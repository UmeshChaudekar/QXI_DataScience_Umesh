{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import sparse\n",
    "\n",
    "def loadData(filePath):\n",
    "    data = pd.read_csv(filePath, header=0)\n",
    "    return data[\"Title\"],data[\"Conference\"]\n",
    "\n",
    "def preProcessing(features):\n",
    "    num_titles = features.size\n",
    "    clean_wordlist = []\n",
    "    clean_titles = []\n",
    "    stops = set(stopwords.words('english'))\n",
    "    for i in range( 0, num_titles):\n",
    "        #letters_only = re.sub(\"[^a-zA-Z]\", \" \", features[i]) \n",
    "        words = features[i].lower().split()\n",
    "        words = [w.lower() for w in words if not w in stops]  \n",
    "        clean_wordlist.append(words)\n",
    "        clean_titles.append(\" \".join(words))\n",
    "    return clean_titles, clean_wordlist\n",
    "\n",
    "def getDTMByTFIDF(features,nfeatures):\n",
    "    tfIdf_vectorizer = TfidfVectorizer(max_features=nfeatures)\n",
    "    dtm = tfIdf_vectorizer.fit_transform(features).toarray()\n",
    "    return dtm,tfIdf_vectorizer\n",
    "\n",
    "def featuresByChiSq(features,labels,nFeature=5000):\n",
    "    chi2_model = SelectKBest(chi2,k=nFeature)\n",
    "    dtm = chi2_model.fit_transform(features,labels)\n",
    "    return dtm,chi2_model\n",
    "\n",
    "def featuresByInformationGain(features,labels):\n",
    "    treeCL = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    treeCL = treeCL.fit(features,labels)\n",
    "    transformed_features = SelectFromModel(treeCL,prefit=True).transform(features)\n",
    "    return transformed_features\n",
    "\n",
    "def featuresByLSA(features,ncomponents=100):\n",
    "    svd = TruncatedSVD(n_components=ncomponents)\n",
    "    normalizer =  Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    dtm_lsa = lsa.fit_transform(features)\n",
    "    return dtm_lsa\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            feature_vec = np.add(feature_vec,model[word]) \n",
    "\n",
    "    feature_vec = np.divide(feature_vec,nwords)\n",
    "   \n",
    "    return feature_vec\n",
    "\n",
    "def getAvgFeatureVecs(title, model, num_features):\n",
    "    counter = 0\n",
    "    titleFeatureVecs = np.zeros((len(title), num_features),dtype=\"float32\")\n",
    "    for t in title:\n",
    "        titleFeatureVecs[counter] = makeFeatureVec(t, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return titleFeatureVecs\n",
    "\n",
    "def crossValidate(document_term_matrix,labels,classifier=\"SVM\",nfold=2):\n",
    "    clf = None\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    \n",
    "    if classifier == \"RF\":\n",
    "        clf = RandomForestClassifier()\n",
    "    elif classifier == \"NB\":\n",
    "        clf = MultinomialNB()\n",
    "    elif classifier == \"SVM\":\n",
    "        clf = LinearSVC()\n",
    "    \n",
    "    skf = StratifiedKFold(random_state=0, shuffle=False)\n",
    "\n",
    "    for train_index, test_index in skf.split(document_term_matrix,labels):\n",
    "        X_train, X_test = document_term_matrix[train_index], document_term_matrix[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        model = clf.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        p,r,f,s = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        fscore.append(f)\n",
    "        \n",
    "    return np.mean(precision),np.mean(recall),np.mean(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, labels = loadData(r\"dataset.csv\")\n",
    "processed_titles, processed_titles_wordlist = preProcessing(titles)\n",
    "dtm,vect = getDTMByTFIDF(processed_titles,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisqDtm, chisqModel = featuresByChiSq(dtm,labels,2000)\n",
    "#igDtm = featuresByInformationGain(dtm,labels)\n",
    "#lsaDtm = featuresByLSA(dtm,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 1       # Number of threads to run in parallel\n",
    "context = 8           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda Installed\\lib\\site-packages\\ipykernel_launcher.py:79: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2Vec(processed_titles_wordlist, workers=num_workers, \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "wordVecs = getAvgFeatureVecs(processed_titles_wordlist, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine features from chiSq and word2Vec\n",
    "combinedFeatures = np.hstack([chisqDtm,wordVecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda Installed\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSq Features: 0.7827887082690306 0.7750294182516826 0.7679508748940886\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore = crossValidate(chisqDtm,labels,\"SVM\",10)\n",
    "print (\"ChiSq Features:\",precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda Installed\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSq Features: 0.7724820563607594 0.765453813142657 0.7585161214604428\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore = crossValidate(combinedFeatures,labels,\"SVM\",10)\n",
    "print (\"ChiSq Features:\",precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
